{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dzy00/CHL5230-Final-Project/blob/main/CHL5230_Final_Project_CHANGE_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uOMhs_EQRSW"
      },
      "source": [
        "# Exploratory Data Analysis\n",
        "\n",
        "*   C CHANGE dataset\n",
        "*   Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkqC2hqUQOuW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# read dataset\n",
        "CHANGE_df = pd.read_excel('C-CHANGE Analysis Data v52c.xlsx')\n",
        "CHANGE_df.head()\n",
        "\n",
        "# check dimension of the dataset\n",
        "print(CHANGE_df.shape)\n",
        "\n",
        "# list the columns of the dataframe\n",
        "print(CHANGE_df.columns)\n",
        "\n",
        "# summary statistics\n",
        "description = CHANGE_df.describe()\n",
        "description\n",
        "description.plot(kind='box', subplots=True, layout=(7, 10), figsize=(10, 5), color='#7569c9')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# check missing value\n",
        "print(CHANGE_df.isnull().sum())\n",
        "\n",
        "# exploration\n",
        "CHANGE_df['High CVD Risk'].unique()\n",
        "CHANGE_df['TC_Age_Sex'].unique() # completely NA\n",
        "print(CHANGE_df['Antihypertensive_usage'].isnull().sum())\n",
        "\n",
        "# check if MacroVasc is a combination of MI_Hx, CAD_Hx, Stroke_Hx\n",
        "def MacroVasc_Hx(row):\n",
        "    if row['MI_Hx']==1 or row['CAD_Hx']==1 or row['Stroke_Hx']==1:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "df_temp = CHANGE_df\n",
        "df_temp['MacroVasc_Hx'] = df_temp.apply(MacroVasc_Hx, axis=1)\n",
        "sum((df_temp['MacroVasc']-df_temp['MacroVasc_Hx'])!=0)\n",
        "\n",
        "# median age, BMI, Waist Circumference by group\n",
        "CHANGE_df.groupby(['MacroVasc'])['Age','BMI','WaistCircumference'].median()\n",
        "\n",
        "# median blood pressure, blood sugar, cholesterol etc by group\n",
        "CHANGE_df.groupby(['MacroVasc'])['sBP','dBP','FBS','HDL','LDL','TC','TG','Urin_Alb_Creat_Ratio','TC_HDL_Ratio'].median()\n",
        "\n",
        "\n",
        "########################################################### visualization\n",
        "\n",
        "plt.figure(figsize=(9,5))\n",
        "\n",
        "# correlation matrix between selected variables\n",
        "CHANGE_corr_Fram = CHANGE_df[['Sex', 'Age', 'sBP', 'dBP', 'FBS', 'HDL', 'LDL', 'TC', 'TG',\n",
        "       'DM_HbA1c', 'HBA1C', 'BetaBlocker_usage', 'HeartFailure_Hx', 'CCB_usage',\n",
        "       'Thiazide_usage', 'ACEI_usage', 'ARB_usage', 'AlphaBlocker_usage',\n",
        "       'DihydropyridineCCB_usage', 'Warfarin_usage', 'Antiplatelet_usage',\n",
        "       'TIA_Hx', 'Dyslipidemia_Hx', 'MacroVasc', 'DM_Macrovasc', 'MI_Hx', 'CAD_Hx', 'Stroke_Hx',\n",
        "       'Smoking_Status', 'DM_Hx', 'Hypertension_Hx', 'TC_HDL_Ratio', 'CVD Risk']]\n",
        "CHANGE_corr_other = CHANGE_df[['Sex', 'Age', 'BMI', 'LDL_CVD Risk', 'LDL-CVDRisk-Statin', 'INR',\n",
        "       'Any_AntiHTN-HTN', 'ACE+ARB', 'ACE-or-ARB', 'MI+BB+ACE-or-ARB', 'MI+BB', 'HTN+CAD',\n",
        "       'HTN+CAD+ACE-or-ARB', 'Diuretic_usage', 'Statin_usage', 'DM_Statin', 'DM_ACE/ARB',\n",
        "       'DipyridamoleER_usage', 'Clopidogrel_usage', 'BetaBlocker_usage', 'Antihyper_Peripher_usage',\n",
        "       'Antihyperglycemics_usage', 'ASA_usage', 'HeartFailure_Hx', 'AtrialFibrillation_Hx', 'TIA_Hx',\n",
        "       'Dyslipidemia_Hx', 'BP>160_Macrovasc', 'MacroVasc',  'DM-sBP<130', 'DM-dBP<80',\n",
        "       'Urin_Alb_Creat_Ratio', 'TC_HDL_Ratio', 'CVD Risk']]\n",
        "Mcorr = CHANGE_corr_Fram.corr()\n",
        "cmap = sns.diverging_palette(240, 10, as_cmap=True) # customerize cmap color\n",
        "sns.heatmap(Mcorr, annot=False, cmap=cmap)\n",
        "plt.show()\n",
        "\n",
        "Mcorr = CHANGE_corr_other.corr()\n",
        "cmap = sns.diverging_palette(240, 10, as_cmap=True) # customerize cmap color\n",
        "sns.heatmap(Mcorr, annot=False, cmap=cmap)\n",
        "plt.show()\n",
        "\n",
        "# age\n",
        "sns.boxplot( x=CHANGE_df['MacroVasc'], y=CHANGE_df['Age'], palette=\"Purples\")\n",
        "plt.title('Figure. Distribution of Age by Macrovascular Disease')\n",
        "plt.xlabel('Macrovascular Disease')\n",
        "plt.ylabel('Age')\n",
        "plt.show()\n",
        "\n",
        "# gender\n",
        "# grouped bar chart using seaborn's countplot function\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.countplot(data=CHANGE_df, x='Sex', hue='MacroVasc', palette=['#1c3a73', '#7cb1c2'])\n",
        "plt.xlabel('Gender')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Count of Macrovascular Disease cases for Male and Female')\n",
        "plt.legend(title='Macrovascular Disease')\n",
        "plt.show()\n",
        "\n",
        "# BMI\n",
        "sns.boxplot( x=CHANGE_df['MacroVasc'], y=CHANGE_df['BMI'], palette=\"Blues\")\n",
        "plt.title('Figure. Distribution of BMI by Macrovascular Disease')\n",
        "plt.xlabel('Macrovascular Disease')\n",
        "plt.ylabel('BMI')\n",
        "plt.show()\n",
        "\n",
        "# diabetes\n",
        "# grouped bar chart\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.countplot(data=CHANGE_df, x='DM_HbA1c', hue='MacroVasc', palette=['#1c3a73', '#7cb1c2'])\n",
        "plt.xlabel('Diabetes')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Count of Macrovascular Disease for People With and Without Diabetes')\n",
        "plt.legend(title='Macrovascular Disease')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# distribution of the class MacroVasc\n",
        "import plotly.express as px\n",
        "counts = CHANGE_df['MacroVasc'].value_counts()\n",
        "# Define color sequence\n",
        "colors = ['#1c3a73', '#7cb1c2']\n",
        "fig = px.pie(\n",
        "    names=counts.index,\n",
        "    values=counts.values,\n",
        "    hole=0.5,  # size of the hole, 0.5 means 50%\n",
        "    title='Distribution of Classes in Macrovascular Disease Column',\n",
        "    color_discrete_sequence=colors  # apply colors\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "# plot median CVD Risk over age by gender groups\n",
        "# create pivot table\n",
        "meian_CVDRisk = CHANGE_df.pivot_table(index='Age', columns='Sex', values='CVD Risk', aggfunc='median')\n",
        "# plot\n",
        "plt.figure(figsize=(6, 5))\n",
        "meian_CVDRisk.plot(kind='line')\n",
        "plt.ylabel('median CVD Risk')\n",
        "plt.title('Median CVD Risk Across Age By Genders')\n",
        "plt.show()\n",
        "\n",
        "# distribution of CVD risk\n",
        "plt.hist(CHANGE_df['CVD Risk'], bins=50)\n",
        "plt.gca().set(title='Frequency Distribution of CVD Risk', ylabel='Frequency')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qny3oK-7B-O"
      },
      "outputs": [],
      "source": [
        "# table one: baseline patient characteristics\n",
        "!pip install tableone\n",
        "from tableone import TableOne\n",
        "T1_columns = ['Sex','Age','BMI','WaistCircumference','sBP','dBP','HDL','TC','TG','HBA1C',\n",
        "              'Any_AntiHTN','HeartFailure_Hx','AtrialFibrillation_Hx','Smoking_Status',\n",
        "              'Urin_Alb_Creat_Ratio','Hypertension_Hx','CVD Risk','High CVD Risk']\n",
        "T1_categorical = ['Sex','Any_AntiHTN','HeartFailure_Hx','AtrialFibrillation_Hx','Hypertension_Hx','Smoking_Status',\n",
        "                  'High CVD Risk']\n",
        "T1_group_by = ['MacroVasc']\n",
        "Tableone = TableOne(data=CHANGE_df, columns=T1_columns, categorical=T1_categorical, groupby=T1_group_by, pval=True)\n",
        "print(Tableone.tabulate(tablefmt = \"fancy_grid\"))\n",
        "\n",
        "labels={'MacroVasc': 'MacroVascular Disease'}\n",
        "# group by MacroVasc, 0 column: non MacroVasc disease, 1: has\n",
        "\n",
        "Tableone.to_excel('Tableone raw.xlsx')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGQk33zYJ0AU"
      },
      "source": [
        "\n",
        "---\n",
        "## Data Cleaning (Deal with N/A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBRydoXYQQ5G"
      },
      "outputs": [],
      "source": [
        "########################################################## clean dataset\n",
        "# Transform Sex column\n",
        "CHANGE_df['Sex'].replace({'F' : 1, 'M' : 0}, inplace=True)\n",
        "\n",
        "# selected variables are stored in dataframe CHANGE_selected\n",
        "CHANGE_selected = CHANGE_df[['Sex','Age','BMI','sBP','dBP','HDL','TC','TG','HBA1C',\n",
        "              'Any_AntiHTN','HeartFailure_Hx','AtrialFibrillation_Hx','MacroVasc','Smoking_Status',\n",
        "              'Urin_Alb_Creat_Ratio','Hypertension_Hx']]\n",
        "\n",
        "# Dealing with Missing Value\n",
        "# drop rows with NA in sex\n",
        "CHANGE_selected = CHANGE_selected.dropna(subset=['Sex'])\n",
        "\n",
        "# Imputation\n",
        "# impute NA in categorical variables with 0\n",
        "CHANGE_selected['HeartFailure_Hx'].fillna(0, inplace=True)\n",
        "CHANGE_selected['AtrialFibrillation_Hx'].fillna(0, inplace=True)\n",
        "CHANGE_selected['Smoking_Status'].fillna(0.0, inplace=True)\n",
        "CHANGE_selected['Hypertension_Hx'].fillna(0.0, inplace=True)\n",
        "\n",
        "## OPTION 1\n",
        "# Delete rows with NA in BMI, sBP, dBP, HDL, HBA1C, TC, TG\n",
        "CHANGE_noNA = CHANGE_selected.dropna(subset=['BMI','sBP','dBP','HDL','HBA1C','TC','TG','Urin_Alb_Creat_Ratio'], how='any')\n",
        "CHANGE_noNA['MacroVasc'].sum() # check number of rows with macrovascualr disease\n",
        "\n",
        "# Transform the HBA1C column to 1 for HBA1C>=6.5, 0 for HBA1C<6.5\n",
        "def funDiabetes(row):\n",
        "    if row['HBA1C'] < 6.5 and row['HBA1C'] >= 0:\n",
        "        return 0\n",
        "    elif row['HBA1C'] >= 6.5:\n",
        "        return 1\n",
        "    else:\n",
        "        pass\n",
        "CHANGE_noNA['diabetes'] = CHANGE_noNA.apply(funDiabetes, axis=1)\n",
        "# Drop HBA1C\n",
        "CHANGE_noNA = CHANGE_noNA.drop(columns=['HBA1C'])\n",
        "\n",
        "# Calculate the percentage of rows with one or more missing values\n",
        "# percentage_missing_rows = (CHANGE_selected.isnull().sum(axis=1) > 0).sum() / len(CHANGE_selected)\n",
        "# percentage_missing_rows\n",
        "\n",
        "# Table one of cleaned dataset\n",
        "T1_columns = ['Sex','Age','BMI','sBP','dBP','HDL','TC','TG','diabetes',\n",
        "              'Any_AntiHTN','HeartFailure_Hx','AtrialFibrillation_Hx','Smoking_Status',\n",
        "              'Urin_Alb_Creat_Ratio','Hypertension_Hx']\n",
        "T1_categorical = ['Sex','Any_AntiHTN','HeartFailure_Hx','AtrialFibrillation_Hx','Hypertension_Hx','Smoking_Status','diabetes']\n",
        "T1_group_by = ['MacroVasc']\n",
        "Table = TableOne(data=CHANGE_noNA, columns=T1_columns, categorical=T1_categorical, groupby=T1_group_by, pval=True)\n",
        "print(Table.tabulate(tablefmt = \"fancy_grid\"))\n",
        "\n",
        "Tableone.to_excel('Tableone no NA.xlsx')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAOLKZGIjmB_"
      },
      "outputs": [],
      "source": [
        "## OPTION 2\n",
        "# impute continuous variables with median\n",
        "cols = ['BMI','sBP','dBP','HDL','HBA1C','TC','TG','Urin_Alb_Creat_Ratio']\n",
        "CHANGE_medianImpute = CHANGE_selected.fillna(CHANGE_selected[cols].median())\n",
        "\n",
        "# transform the HBA1C column to 1 for HBA1C>=6.5, 0 for HBA1C<6.5\n",
        "def funDiabetes(row):\n",
        "    if row['HBA1C'] < 6.5 and row['HBA1C'] >= 0:\n",
        "        return 0\n",
        "    elif row['HBA1C'] >= 6.5:\n",
        "        return 1\n",
        "    else:\n",
        "        pass\n",
        "CHANGE_medianImpute['diabetes'] = CHANGE_medianImpute.apply(funDiabetes, axis=1)\n",
        "# drop HBA1C\n",
        "CHANGE_medianImpute = CHANGE_medianImpute.drop(columns=['HBA1C'])\n",
        "\n",
        "# table one of cleaned dataset\n",
        "T1_columns = ['Sex','Age','BMI','sBP','dBP','HDL','TC','TG','diabetes',\n",
        "              'Any_AntiHTN','HeartFailure_Hx','AtrialFibrillation_Hx','Smoking_Status',\n",
        "              'Urin_Alb_Creat_Ratio','Hypertension_Hx']\n",
        "T1_categorical = ['Sex','Any_AntiHTN','HeartFailure_Hx','AtrialFibrillation_Hx','Hypertension_Hx','Smoking_Status','diabetes']\n",
        "T1_group_by = ['MacroVasc']\n",
        "Tableone = TableOne(data=CHANGE_medianImpute, columns=T1_columns, categorical=T1_categorical, groupby=T1_group_by, pval=True)\n",
        "print(Tableone.tabulate(tablefmt = \"fancy_grid\"))\n",
        "\n",
        "Tableone.to_excel('Tableone median impute.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gT1wIiau_LJ"
      },
      "outputs": [],
      "source": [
        "# cleaned dataset name\n",
        "# median imputer: CHANGE_medianImpute\n",
        "# delete NA: CHANGE_noNA\n",
        "\n",
        "# correlation plot on cleaned dataset\n",
        "plt.title('Correlation Matrix')\n",
        "Mcorr = CHANGE_noNA.corr()\n",
        "cmap = sns.diverging_palette(240, 10, as_cmap=True) # customerize cmap color\n",
        "sns.heatmap(Mcorr, annot=False, cmap=cmap)\n",
        "plt.show()\n",
        "\n",
        "######################################### export cleaned datasets for later analysis\n",
        "CHANGE_noNA.to_csv(\"CHANGE_noNA.csv\")\n",
        "CHANGE_medianImpute.to_csv(\"CHANGE_medianImpute.csv\")\n",
        "\n",
        "# transform the HBA1C column to 1 for HBA1C>=6.5, 0 for HBA1C<6.5 in the selected dataset\n",
        "def funDiabetes(row):\n",
        "    if row['HBA1C'] < 6.5 and row['HBA1C'] >= 0:\n",
        "        return 0\n",
        "    elif row['HBA1C'] >= 6.5:\n",
        "        return 1\n",
        "    else:\n",
        "        pass\n",
        "CHANGE_selected['diabetes'] = CHANGE_selected.apply(funDiabetes, axis=1)\n",
        "# drop HBA1C\n",
        "CHANGE_selected = CHANGE_selected.drop(columns=['HBA1C'])\n",
        "\n",
        "# export\n",
        "CHANGE_selected.to_csv(\"CHANGE_selected.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE8Bg7sItAD5"
      },
      "source": [
        "# Apply the K-Means Method\n",
        "Using data with removed missing values. Dataset name: CHANGE_noNA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOco1I0H9v8q"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for K-Fold Cross-Validation for K-means\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "# Features for clustering\n",
        "X = CHANGE_noNA.drop(['MacroVasc'], axis = 1)\n",
        "\n",
        "# Range of K values to test\n",
        "k_values = range(2, 7)\n",
        "\n",
        "# Number of folds for K-Fold Cross-Validation\n",
        "n_folds = 5\n",
        "\n",
        "# Initialize a list to store evaluation metrics\n",
        "scores = []\n",
        "\n",
        "# Iterate through K values and perform Cross-Validation\n",
        "for k in k_values:\n",
        "  kmeans = KMeans(n_clusters=k, random_state=0)\n",
        "  kf = KFold(n_splits=n_folds, shuffle=True, random_state=0)\n",
        "  fold_scores = []\n",
        "  for train_idx, test_idx in kf.split(X):\n",
        "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "    kmeans.fit(X_train)\n",
        "    labels = kmeans.predict(X_test)\n",
        "\n",
        "# Calculate a clustering quality metric (using Silhouette Score)\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "score = silhouette_score(X_test, labels)\n",
        "fold_scores.append(score)\n",
        "mean_score = np.mean(fold_scores)\n",
        "scores.append(mean_score)\n",
        "\n",
        "# Choose the best K value\n",
        "best_k = k_values[np.argmax(scores)]\n",
        "print(f\"The best K value is {best_k}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPwGcgYMLK2w"
      },
      "outputs": [],
      "source": [
        "# Required libraries for K-means Implementation\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to visualize the clustering results\n",
        "def plot_clusters(x, labels, k):\n",
        "    # Define colors for each cluster\n",
        "    colors = ['#860991', '#d6ba2f', '#c4c1b3']\n",
        "    # Loop through each cluster\n",
        "    for i in range(k):\n",
        "        # Filter the data points that belong to the current cluster\n",
        "        filtered_label0 = x[labels == i]\n",
        "        # Plot the filtered data points using the corresponding color\n",
        "        plt.scatter(filtered_label0[:, 0], filtered_label0[:, 1], c=colors[i])\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "# Choose features for clustering and visualization\n",
        "features = ['Sex','Age','BMI','sBP','dBP','HDL','TC','TG','diabetes',\n",
        "              'Any_AntiHTN','HeartFailure_Hx','AtrialFibrillation_Hx','Smoking_Status',\n",
        "              'Urin_Alb_Creat_Ratio','Hypertension_Hx']\n",
        "\n",
        "# Convert the selected features from DataFrame to a NumPy array for easier processing with sklearn\n",
        "# Choose any two features we are interested in from the list of features for clustering\n",
        "data = CHANGE_noNA[['dBP','HDL']].to_numpy()\n",
        "\n",
        "# Standardize the data - this makes the mean 0 and standard deviation 1 for each feature\n",
        "# Standardization can help the k-means algorithm perform better as it's sensitive to feature scales\n",
        "st_x = StandardScaler()\n",
        "data = st_x.fit_transform(data)\n",
        "\n",
        "# Apply k-means clustering on the standardized data\n",
        "# Use the best k value 2\n",
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "label = kmeans.fit_predict(data)\n",
        "\n",
        "# Visualize the clustering results using the defined function\n",
        "plot_clusters(data, label, 2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display pair plot for data visualization\n",
        "sns.pairplot(CHANGE_noNA.iloc[:,1:len(CHANGE_noNA.columns)], hue='MacroVasc', palette='magma')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6zS-ApEbz7cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTpTYiSIs-Wf"
      },
      "source": [
        "# Apply the Logistic Regression (Method 1)\n",
        "Using data with removed missing values. Dataset name: CHANGE_noNA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGDXkHpcKUsp"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Store the explanatory and target features\n",
        "# X = CHANGE_noNA.drop('MacroVasc', axis = 1)\n",
        "# y = CHANGE_noNA['MacroVasc']\n",
        "\n",
        "X = CHANGE_medianImpute.drop('MacroVasc', axis = 1)\n",
        "y = CHANGE_medianImpute['MacroVasc']\n",
        "\n",
        "# # Normalizing data\n",
        "# scaler = StandardScaler()\n",
        "# X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Number of folds for cross-validation and define kfold\n",
        "n_folds = 5\n",
        "kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform K-fold cross-validation and obtain the accuracy scores for each fold\n",
        "for train_index, test_index in kfold.split(X, y):\n",
        "  X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "  y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "  model = LogisticRegression()\n",
        "  model.fit(X_train, y_train)\n",
        "  predictions = model.predict(X_test)\n",
        "  accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "  print(f'Fold Accuracy: {accuracy}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lT0aVLkbxxiB"
      },
      "outputs": [],
      "source": [
        "# Tomelinks Undersampling\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "\n",
        "tl = TomekLinks()\n",
        "X_resampled, y_resampled = tl.fit_resample(X_train, y_train)\n",
        "\n",
        "# Training logistic regression model\n",
        "log_reg = LogisticRegression(penalty='l2', C=1.0, solver='liblinear')\n",
        "log_reg.fit(X_resampled, y_resampled)\n",
        "\n",
        "# Use the model to make predictions on the testing data\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report)\n",
        "\n",
        "# Visualizing confusion matrix\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()\n",
        "\n",
        "import statsmodels.api as sm\n",
        "import pandas as pd\n",
        "\n",
        "# Separate features and target variable\n",
        "# X = CHANGE_noNA.drop('MacroVasc', axis = 1)\n",
        "# y = CHANGE_noNA['MacroVasc']\n",
        "\n",
        "X = CHANGE_medianImpute.drop('MacroVasc', axis = 1)\n",
        "y = CHANGE_medianImpute['MacroVasc']\n",
        "\n",
        "# Add a constant term for the intercept\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Create and fit logistic regression model\n",
        "log_reg = sm.Logit(y, X).fit()\n",
        "\n",
        "# Get summary report\n",
        "summary = log_reg.summary()\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploration of the Features"
      ],
      "metadata": {
        "id": "Nao73DbonFXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create logistic regression object\n",
        "log_reg = LogisticRegression()\n",
        "\n",
        "fig, axs = plt.subplots(4, 4, figsize=(20, 10))\n",
        "fig.tight_layout(pad=5.0)  # Adds padding between subplots\n",
        "\n",
        "# select_col = CHANGE_noNA.columns[0:11].append(CHANGE_noNA.columns[12:16])\n",
        "select_col = CHANGE_medianImpute.columns[0:11].append(CHANGE_medianImpute.columns[12:16])\n",
        "\n",
        "for i, feature in enumerate(select_col):  # Exclude 'Outcome' column\n",
        "    row = i // 4  # Determine row index\n",
        "    col = i % 4   # Determine column index\n",
        "\n",
        "    # Reshape the feature and outcome arrays to fit the model\n",
        "    # X_plot = CHANGE_noNA[[feature]]\n",
        "    # y_plot = CHANGE_noNA['MacroVasc']\n",
        "    X_plot = CHANGE_medianImpute[[feature]]\n",
        "    y_plot = CHANGE_medianImpute['MacroVasc']\n",
        "\n",
        "    # Fit logistic regression model\n",
        "    log_reg.fit(X_plot, y)\n",
        "\n",
        "    # Get min and max values of the feature for plotting\n",
        "    x_test = np.linspace(X_plot.min(), X_plot.max(), 300)\n",
        "\n",
        "    # Get the probability for each value in the range\n",
        "    prob = log_reg.predict_proba(x_test)[:, 1]\n",
        "\n",
        "    # Plot the data points\n",
        "    axs[row, col].scatter(X_plot,  y_plot, color='#7569c9', label='Data points')\n",
        "\n",
        "    # Plot the sigmoid curve\n",
        "    axs[row, col].plot(x_test, prob, color='#e0d122', lw=3, label='Logistic regression curve' )\n",
        "\n",
        "    axs[row, col].set_title(f'Logistic Regression Curve - {feature}')\n",
        "    axs[row, col].set_xlabel(feature)\n",
        "    axs[row, col].set_ylabel('Probability')\n",
        "    axs[row, col].legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yA3EBD59KdgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply the Logistic Regression (Method 2)\n",
        "Using data with removed missing values. Dataset name: CHANGE_noNA"
      ],
      "metadata": {
        "id": "QsbC17gf8EpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for feature selection and logistic regression\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Store the explanatory and target features\n",
        "# X = CHANGE_noNA.drop('MacroVasc', axis = 1)\n",
        "# y = CHANGE_noNA['MacroVasc']\n",
        "X = CHANGE_medianImpute.drop('MacroVasc', axis = 1)\n",
        "y = CHANGE_medianImpute['MacroVasc']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "# Tomelinks Undersampling\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "\n",
        "t1 = TomekLinks()\n",
        "X_train, y_train = t1.fit_resample(X_train, y_train)\n",
        "\n",
        "# Initialize a Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Define a StratifiedKFold cross-validation\n",
        "cv = StratifiedKFold(3)\n",
        "\n",
        "# Initialize Recursive Feature Elimination with Cross-Validation (RFECV)\n",
        "rfecv = RFECV(\n",
        "    estimator=model,\n",
        "    step=1,\n",
        "    cv=cv,\n",
        "    scoring='roc_auc',  # Specify the scoring metric (ROC AUC in this case)\n",
        "    min_features_to_select=1,\n",
        ")\n",
        "rfecv.fit(X_train, y_train)\n",
        "\n",
        "# Print the optimal number of selected features\n",
        "print(f\"Optimal number of features: {rfecv.n_features_}\")"
      ],
      "metadata": {
        "id": "TiZ_scjAXljW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# which columns used and which wasn't\n",
        "rfecv.support_\n"
      ],
      "metadata": {
        "id": "R4-3QANPc2sM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# features ranking low is better\n",
        "rfecv.ranking_\n"
      ],
      "metadata": {
        "id": "fh1pQbhyc7s5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of different feature selections tested\n",
        "n_scores = len(rfecv.cv_results_[\"mean_test_score\"])\n",
        "\n",
        "# Create a new figure for plotting\n",
        "plt.figure()\n",
        "\n",
        "# Set labels for the x and y axes\n",
        "plt.xlabel(\"Number of features selected\")\n",
        "plt.ylabel(\"Mean test score\")\n",
        "\n",
        "# Plot the mean test scores with error bars\n",
        "plt.errorbar(\n",
        "    range(1, n_scores + 1),\n",
        "    rfecv.cv_results_[\"mean_test_score\"],\n",
        "    yerr=rfecv.cv_results_[\"std_test_score\"],\n",
        "    color='#995687'  # Change the color to green\n",
        ")\n",
        "\n",
        "# Set the title for the plot\n",
        "plt.title(\"Recursive Feature Elimination\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oCImHuH2dDw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to compute various scores for model evaluation\n",
        "def get_scores(Y_pred, Y):\n",
        "    # Calculate confusion matrix, classification report, ROC AUC, and accuracy\n",
        "    conf_matrix = confusion_matrix(Y_pred, Y)\n",
        "    class_report = classification_report(Y_pred, Y)\n",
        "    auc = roc_auc_score(Y_pred, Y)\n",
        "    acc = accuracy_score(Y_pred, Y)\n",
        "\n",
        "    # Return the computed scores\n",
        "    return conf_matrix, class_report, auc, acc\n",
        "\n",
        "# Define a function to print the computed scores\n",
        "def print_scores(conf_matrix, class_report, auc, acc):\n",
        "    print('AUC : ', auc)\n",
        "    print('Accuracy : ', acc)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(class_report)\n"
      ],
      "metadata": {
        "id": "PDZKuLcJkdKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_columns = X_train.columns[rfecv.support_]\n",
        "\n",
        "# Select only the features that were chosen by RFECV\n",
        "X_train = X_train[selected_columns]\n",
        "X_test = X_test[selected_columns]\n",
        "\n",
        "# Initialize and train a Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test and training sets\n",
        "y_pred_test = model.predict(X_test)\n",
        "y_pred_train = model.predict(X_train)\n",
        "\n",
        "# Calculate evaluation metrics and scores for the test and training sets\n",
        "conf_matrix_test, class_report_test, auc_test, acc_test = get_scores(y_pred_test, y_test)\n",
        "conf_matrix_train, class_report_train, auc_train, acc_train = get_scores(y_pred_train, y_train)\n",
        "\n",
        "# Print scores and evaluation metrics for the training set\n",
        "print('======== Training Set ==========')\n",
        "print_scores(conf_matrix_train, class_report_train, auc_train, acc_train)\n",
        "\n",
        "# Print scores and evaluation metrics for the test set\n",
        "print('======== Test Set ==========')\n",
        "print_scores(conf_matrix_test, class_report_test, auc_test, acc_test)\n",
        "\n",
        "# Plot the ROC curve for the training set\n",
        "from sklearn.metrics import RocCurveDisplay, auc, roc_curve\n",
        "fpr, tpr, thresholds = roc_curve(y_pred_train, y_train)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='Logistic Regression')\n",
        "display.plot()"
      ],
      "metadata": {
        "id": "BPTqbi4QgDoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree\n",
        "\n",
        "1.   split data into 80% train, 20% test\n",
        "2.   impute continuous variable with median, categorical with 0/or mode\n",
        "3.   undersample major class with TomeKLinks\n",
        "4.   Fit an initial model (step 2~4 are in pipeline)\n",
        "5.   Tune hyperparameter using Grid search with cross validation\n",
        "6.   Update model with the best hyperparameter\n",
        "7.   Evaluate model performance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SxI5nhjFkhyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CHL5230 final project C CHANGE dataset\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "import plotly.express as px\n",
        "from category_encoders import OneHotEncoder, TargetEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "#from sklearn.pipeline import Pipeline\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.under_sampling import TomekLinks, RandomUnderSampler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "dir = \"/Users/zding/Documents/zd/notes/Biostatistics/2023 fall/CHL 5230/final project/C CHANGE code\"\n",
        "# read the cleaned dataset, so we don't have to rerun the above exploratory and cleaned code\n",
        "CHANGE_selected = pd.read_csv('CHANGE_selected.csv')\n",
        "\n",
        "\n",
        "# Define the column that will be used as the target for modeling or analysis\n",
        "target_column = 'MacroVasc'\n",
        "\n",
        "# List of columns that contain categorical data\n",
        "categorical_columns = ['Sex','Any_AntiHTN','HeartFailure_Hx','AtrialFibrillation_Hx',\n",
        "                       'Smoking_Status','Hypertension_Hx','diabetes']\n",
        "\n",
        "# List of columns that contain numerical data (excluding categorical columns and the target column)\n",
        "numerical_columns = [c for c in CHANGE_selected.columns if c not in categorical_columns and c != target_column]\n",
        "\n",
        "\n",
        "########################################################## classification tree\n",
        "# Splitting the data into 80% training and 20% testing\n",
        "train = CHANGE_selected.sample(frac=0.8, random_state=10)\n",
        "test = CHANGE_selected.drop(train.index)\n",
        "\n",
        "# separate X, y\n",
        "X_train = train.drop('MacroVasc', axis = 1)\n",
        "y_train = train['MacroVasc']\n",
        "\n",
        "# imputate numerical columns with median\n",
        "num_transformer = SimpleImputer(strategy = 'median')\n",
        "\n",
        "# impute categorical columns with the most frequent value and encode\n",
        "cat_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
        "    #('one_hot_encoder', OneHotEncoder())\n",
        "    ])\n",
        "\n",
        "# initialize a column transformer that handels imputation and encoding\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', num_transformer, numerical_columns),\n",
        "        ('cat', cat_transformer, categorical_columns)\n",
        "        ],\n",
        "    remainder='passthrough')\n",
        "\n",
        "\n",
        "##### model fitting\n",
        "# Initializing a Decision Tree classifier\n",
        "dt = tree.DecisionTreeClassifier(criterion='entropy',class_weight='balanced')\n",
        "\n",
        "# Creating a pipeline that first preprocesses the data and then applies the decision tree\n",
        "# this pipeline perform imputation, encoding, undersampling, and model fitting\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('tomek', TomekLinks()),\n",
        "    #('under_sample', RandomUnderSampler()),\n",
        "    ('DT', dt)\n",
        "])\n",
        "\n",
        "# Display the pipeline architecture\n",
        "model\n",
        "\n",
        "# Training the model on the training data\n",
        "model = model.fit(X_train, y_train)\n",
        "\n",
        "# Predicting and evaluating the model's performance on the training data\n",
        "Y_pred = model.predict(X_train)\n",
        "accuracy_score(y_train, Y_pred)\n",
        "\n",
        "# Predicting and evaluating the model's performance on the test data\n",
        "Y_pred = model.predict(test.drop('MacroVasc', axis=1))\n",
        "accuracy_score(test['MacroVasc'], Y_pred)\n",
        "\n",
        "# confusion matrix on test data\n",
        "Y_test = test['MacroVasc']\n",
        "conf_matrix_Tree = confusion_matrix(Y_test, Y_pred)\n",
        "class_report_Tree = classification_report(Y_test, Y_pred)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report_Tree)\n",
        "\n",
        "sns.heatmap(conf_matrix_Tree, cmap='Blues', annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Figure. Confusion Matrix for Classification Tree model')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "### hyperparameter tuning using grid search\n",
        "# Define the hyperparameter values that should be tested\n",
        "param_dist = {\n",
        "    \"DT__max_depth\" : [5, 10, 15, 20 ,25, 30, 35],\n",
        "    \"DT__min_samples_leaf\" : [5, 10, 15, 20 ,25],\n",
        "    \"DT__splitter\" : ['best', 'random'],\n",
        "    \"DT__max_features\" : ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Setting up Stratified K-Fold cross-validation\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "\n",
        "# Initialize Grid Search with the model and the hyperparameters to search\n",
        "random_search = GridSearchCV(\n",
        "    model, param_grid=param_dist, cv=skf\n",
        ")\n",
        "\n",
        "# Train Grid Search on the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Extract results into a DataFrame\n",
        "results = pd.DataFrame(random_search.cv_results_)\n",
        "\n",
        "# Display the set of parameters that achieved the best score\n",
        "results[results['rank_test_score'] == 1]\n",
        "\n",
        "# Retrieve the best hyperparameters after Grid Search\n",
        "random_search.best_params_\n",
        "\n",
        "##### fit classification tree with optimal hyperparameter\n",
        "# Update the model's hyperparameters to the best found during Grid Search\n",
        "model = model.set_params(**random_search.best_params_)\n",
        "\n",
        "# Refit the model using the training data\n",
        "model = model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the training set and compute the accuracy\n",
        "Y_pred = model.predict(X_train)\n",
        "train_accuracy = accuracy_score(y_train, Y_pred)\n",
        "print(train_accuracy)\n",
        "\n",
        "# Make predictions on the test set and compute the accuracy\n",
        "Y_pred = model.predict(test.drop('MacroVasc', axis=1))\n",
        "test_accuracy = accuracy_score(test['MacroVasc'], Y_pred)\n",
        "print(test_accuracy)\n",
        "\n",
        "# confusion matrix on test data\n",
        "Y_test = test['MacroVasc']\n",
        "conf_matrix_Tree = confusion_matrix(Y_test, Y_pred)\n",
        "class_report_Tree = classification_report(Y_test, Y_pred)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report_Tree)\n",
        "\n",
        "sns.heatmap(conf_matrix_Tree, cmap='Blues', annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Figure. Confusion Matrix for Classification Tree model with tuned hyperparameter')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "l8BQeVWJJFap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "zWBCLWVLvpOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read the cleaned dataset\n",
        "CHANGE_selected = pd.read_csv('CHANGE_selected.csv')\n",
        "\n",
        "\n",
        "# Define the column that will be used as the target for modeling or analysis\n",
        "target_column = 'MacroVasc'\n",
        "\n",
        "# List of columns that contain categorical data\n",
        "categorical_columns = ['Sex','Any_AntiHTN','HeartFailure_Hx','AtrialFibrillation_Hx',\n",
        "                       'Smoking_Status','Hypertension_Hx','diabetes']\n",
        "\n",
        "# List of columns that contain numerical data (excluding categorical columns and the target column)\n",
        "numerical_columns = [c for c in CHANGE_selected.columns if c not in categorical_columns and c != target_column]\n",
        "\n",
        "\n",
        "############################# random forest\n",
        "# Splitting the data into 80% training and 20% testing\n",
        "train = CHANGE_selected.sample(frac=0.8, random_state=10)\n",
        "test = CHANGE_selected.drop(train.index)\n",
        "\n",
        "# separate X, y\n",
        "X_train = train.drop('MacroVasc', axis = 1)\n",
        "y_train = train['MacroVasc']\n",
        "\n",
        "\n",
        "# imputate numerical columns with median\n",
        "num_transformer = SimpleImputer(strategy = 'median')\n",
        "\n",
        "# impute categorical columns with the most frequent value and encode\n",
        "cat_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
        "    #('one_hot_encoder', OneHotEncoder())\n",
        "    ])\n",
        "\n",
        "# initialize a column transformer that handels imputation and encoding\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', num_transformer, numerical_columns),\n",
        "        ('cat', cat_transformer, categorical_columns)\n",
        "        ],\n",
        "    remainder='passthrough')\n",
        "\n",
        "\n",
        "##### Initialize a Random Forest classifier\n",
        "# The criterion 'entropy' measures the quality of a split by how mixed the classes are in two groups\n",
        "# class_weight 'balanced_subsample' is used to handle imbalanced classes. It computes weights based on the bootstrap sample for every tree grown.\n",
        "# oob_score is set to True to use out-of-bag samples to estimate the generalization accuracy\n",
        "random_forest = RandomForestClassifier(criterion='entropy', class_weight='balanced_subsample', oob_score=True)\n",
        "\n",
        "# Creating a pipeline that fits the Random Forest model\n",
        "# this pipeline perform imputation, undersampling, and model fitting\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('tomek', TomekLinks()),\n",
        "    #('under_sample', RandomUnderSampler()),\n",
        "    ('random_forest', random_forest)\n",
        "])\n",
        "\n",
        "# Display the pipeline architecture\n",
        "model\n",
        "\n",
        "# Using the pipeline model to train on the training data.\n",
        "model = model.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the target variable on the training set and evaluating the model's accuracy.\n",
        "Y_pred = model.predict(X_train)\n",
        "accuracy_score(y_train, Y_pred)\n",
        "\n",
        "# Predicting the target variable on the test set and evaluating the model's accuracy.\n",
        "Y_pred = model.predict(test.drop('MacroVasc', axis = 1))\n",
        "accuracy_score(test['MacroVasc'], Y_pred)\n",
        "\n",
        "# confusion matrix on test data before tuning\n",
        "Y_test = test['MacroVasc']\n",
        "conf_matrix_Tree = confusion_matrix(Y_test, Y_pred)\n",
        "class_report_Tree = classification_report(Y_test, Y_pred)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report_Tree)\n",
        "\n",
        "sns.heatmap(conf_matrix_Tree, cmap='Blues', annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Figure. Confusion Matrix for Random Forest before tuning')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "##### hyperparameter tuning with grid search\n",
        "# Setting hyperparameters for Random Forest for the Grid Search.\n",
        "param_dist = {\n",
        "    \"random_forest__n_estimators\" : [200],\n",
        "    \"random_forest__max_depth\" : [5, 10, 15],\n",
        "    \"random_forest__min_samples_leaf\" : [15, 20 ,25],\n",
        "    \"random_forest__max_features\" : ['sqrt']\n",
        "}\n",
        "\n",
        "# Initializing stratified K-fold cross-validation. This ensures each fold has the same proportion of target labels as the whole dataset.\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "\n",
        "# Setting up GridSearchCV to search through the parameter space.\n",
        "# It will evaluate the model for each combination of hyperparameters provided using stratified K-fold cross-validation.\n",
        "random_search = GridSearchCV(\n",
        "    model, param_grid=param_dist, cv=skf\n",
        ")\n",
        "\n",
        "# Running the GridSearchCV on the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Extracting the results of the Grid Search into a DataFrame for analysis.\n",
        "results = pd.DataFrame(random_search.cv_results_)\n",
        "\n",
        "# Filtering to get the rows with the best performing hyperparameters.\n",
        "results[results['rank_test_score'] == 1]\n",
        "\n",
        "# Updating the model's hyperparameters with the best parameters identified from the Grid Search.\n",
        "model = model.set_params(**random_search.best_params_)\n",
        "\n",
        "# Retraining the model with the updated hyperparameters on the training data.\n",
        "model = model.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the target variable on the training set and evaluating the model's accuracy.\n",
        "Y_pred = model.predict(X_train)\n",
        "accuracy_score(y_train, Y_pred)\n",
        "\n",
        "# Predicting the target variable on the test set and evaluating the model's accuracy.\n",
        "Y_pred = model.predict(test.drop('MacroVasc', axis = 1))\n",
        "accuracy_score(test['MacroVasc'], Y_pred)\n",
        "\n",
        "# confusion matrix on test data before tuning\n",
        "Y_test = test['MacroVasc']\n",
        "conf_matrix_forest = confusion_matrix(Y_test, Y_pred)\n",
        "class_report_forest = classification_report(Y_test, Y_pred)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report_forest)\n",
        "\n",
        "sns.heatmap(conf_matrix_forest, cmap='Blues', annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Figure. Confusion Matrix for Random Forest with the best hyperparameter after tuning')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "TmDoDx-QvISd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost Classifier"
      ],
      "metadata": {
        "id": "G73S5FtuvSxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read the cleaned dataset\n",
        "CHANGE_selected = pd.read_csv('CHANGE_selected.csv')\n",
        "\n",
        "\n",
        "# Define the column that will be used as the target for modeling or analysis\n",
        "target_column = 'MacroVasc'\n",
        "\n",
        "# List of columns that contain categorical data\n",
        "categorical_columns = ['Sex','Any_AntiHTN','HeartFailure_Hx','AtrialFibrillation_Hx',\n",
        "                       'Smoking_Status','Hypertension_Hx','diabetes']\n",
        "\n",
        "# List of columns that contain numerical data (excluding categorical columns and the target column)\n",
        "numerical_columns = [c for c in CHANGE_selected.columns if c not in categorical_columns and c != target_column]\n",
        "\n",
        "\n",
        "######################################## XGBoost\n",
        "# Splitting the data into 80% training and 20% testing\n",
        "train = CHANGE_selected.sample(frac=0.8, random_state=10)\n",
        "test = CHANGE_selected.drop(train.index)\n",
        "\n",
        "# separate X, y\n",
        "X_train = train.drop('MacroVasc', axis = 1)\n",
        "y_train = train['MacroVasc']\n",
        "\n",
        "# imputate numerical columns with median\n",
        "num_transformer = SimpleImputer(strategy = 'median')\n",
        "\n",
        "# impute categorical columns with the most frequent value and encode\n",
        "cat_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
        "    #('one_hot_encoder', OneHotEncoder())\n",
        "    ])\n",
        "\n",
        "# initialize a column transformer that handels imputation and encoding\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', num_transformer, numerical_columns),\n",
        "        ('cat', cat_transformer, categorical_columns)\n",
        "        ],\n",
        "    remainder='passthrough')\n",
        "\n",
        "\n",
        "# Initializing the Gradient Boosting Classifier with specified parameters\n",
        "xg_boost = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)\n",
        "\n",
        "# Creating a Pipeline: Gradient Boosting model\n",
        "# impute, downsample, xg boost\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('tomek', TomekLinks()),\n",
        "    #('under_sample', RandomUnderSampler()),\n",
        "    ('xg_boost', xg_boost) # Training/prediction step: Using Gradient Boosting\n",
        "])\n",
        "\n",
        "# Display the pipeline architecture\n",
        "model\n",
        "\n",
        "# Training the Gradient Boosting model on the training dataset\n",
        "model = model.fit(X_train, y_train)\n",
        "\n",
        "# Predicting on the training dataset and computing the accuracy\n",
        "Y_pred = model.predict(X_train)\n",
        "accuracy_score(y_train, Y_pred)\n",
        "\n",
        "# Predicting on the test dataset and computing the accuracy\n",
        "Y_pred = model.predict(test.drop('MacroVasc', axis = 1))\n",
        "accuracy_score(test['MacroVasc'], Y_pred)\n",
        "\n",
        "# confusion matrix on test data before tuning\n",
        "Y_test = test['MacroVasc']\n",
        "conf_matrix_forest = confusion_matrix(Y_test, Y_pred)\n",
        "class_report_forest = classification_report(Y_test, Y_pred)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report_forest)\n",
        "\n",
        "sns.heatmap(conf_matrix_forest, cmap='Blues', annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Figure. Confusion Matrix for XGBooster')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "##### hyperparameter tuning\n",
        "# Initializing the Gradient Boosting model with initial parameters\n",
        "xg_boost = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)\n",
        "\n",
        "# Creating a pipeline that first applies the column transformations, downsampling, and then runs the Gradient Boosting model\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('tomek', TomekLinks()),\n",
        "    #('under_sample', RandomUnderSampler()),\n",
        "    ('xg_boost', xg_boost) # Training/prediction step: Using Gradient Boosting\n",
        "])\n",
        "\n",
        "# Defining the hyperparameters to be tuned using GridSearchCV\n",
        "param_dist = {\n",
        "    \"xg_boost__n_estimators\" : [100, 200],\n",
        "    \"xg_boost__max_depth\" : [1, 5],\n",
        "    \"xg_boost__min_samples_leaf\" : [25],\n",
        "    \"xg_boost__learning_rate\" : [.1,  .2]\n",
        "}\n",
        "\n",
        "# Using StratifiedKFold for cross-validation, ensuring each fold has the same proportion of observations with each target value\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "\n",
        "# Setting up the GridSearchCV to find the best hyperparameters for the Gradient Boosting model\n",
        "random_search = GridSearchCV(\n",
        "    model, param_grid=param_dist, cv=skf\n",
        ")\n",
        "\n",
        "# Fitting the GridSearchCV on the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Storing and displaying the results of the grid search\n",
        "results = pd.DataFrame(random_search.cv_results_)\n",
        "results[results['rank_test_score'] == 1]\n",
        "\n",
        "### model with the best hyperparameter\n",
        "# Updating the model's parameters with the best ones found from GridSearchCV\n",
        "model = model.set_params(**random_search.best_params_)\n",
        "\n",
        "# Retraining the model with the best parameters on the training dataset\n",
        "model = model.fit(X_train, y_train)\n",
        "\n",
        "# Predicting on the training dataset and computing the accuracy\n",
        "Y_pred = model.predict(X_train)\n",
        "accuracy_score(y_train, Y_pred)\n",
        "\n",
        "# Predicting on the test dataset and computing the accuracy\n",
        "Y_pred = model.predict(test.drop('MacroVasc', axis = 1))\n",
        "accuracy_score(test['MacroVasc'], Y_pred)\n",
        "\n",
        "# confusion matrix on test data before tuning\n",
        "Y_test = test['MacroVasc']\n",
        "conf_matrix_xgboost = confusion_matrix(Y_test, Y_pred)\n",
        "class_report_xgboost = classification_report(Y_test, Y_pred)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report_xgboost)\n",
        "\n",
        "sns.heatmap(conf_matrix_xgboost, cmap='Blues', annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Figure. Confusion Matrix for XGBoost Classifier with the best hyperparameter after tuning')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "PZHS7E0ovRR3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}